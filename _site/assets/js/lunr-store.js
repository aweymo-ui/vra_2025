var store = [ 
    
    
    { 
        "url": "/vra_2025/content/1_intro.html",
        "title": "Introduction",
        "text": "My name is Andrew Weymouth, the Digital Initiatives Librarian for University of Idaho and today I’ll be discussing two practical applications implementing computer vision that can facilitate processing large numbers of digital visual resources. &#10042; The team I’m a part of in the Center for Digital Initiatives and Learning consists of three other people and a small group of student workers, collaborating with the five members of our Special Collections department to create and maintain the 150 digital collections on our main institutional repository as well as the many other, more customized fellowship projects. I also develop digital tools and workflows to enhance transcription, tagging, and image processing, in order to make the university’s audio, text, and visual resources more discoverable for researchers. The U of I Digital Collections, moving from the main browse page to a collection and down to the item level Both of the tools I’ll be speaking about today have their origins in finding practical solutions to repetitive, digital work that comes after the hands-on arranging, research, cataloguing and description work in the archive. Considering I work with technology quite a bit, I would broadly self identify as critical towards AI applications to cultural heritage work, which I think you’ll find reflective in the coding approach, where sets of algorithms and neural networks are implemented on guardrails within a programmatic workflow, to make results more scalable and reproducible. Definitions It might be helpful to begin with some definitions of what resources these tools implement. Both tools are written in Python, a general purpose, readable programming language, and they are hosted in GitHub, a version controlled platform for hosting software development projects. Representation of algorithmic shared characteristics The first tool I’ll discuss implements an artificial intelligence resource. At the moment, most of us would associate AI with large language models that are trained on text and reproduce human language styled output. In contrast, this tool is a more general neural network, computer architecture in which processors are interconnected in a manner suggestive of the human brain. The second tool utilizes machine learning which is a computational method that enables a computer to learn to perform tasks by analyzing a large dataset without being explicitly programmed, in that case to focus on pattern recognition. All of the other resources these tools utilize are Python libraries, wrappers that distribute any of the above models as well as path utilities, which are untrained, deterministic functions."
    },
    { 
        "url": "/vra_2025/content/2_ed.html",
        "title": "edge_detector tool",
        "text": "The first tool was suggested by Evan Williamson, Digital Infrastructure Librarian at the U of I, prompted by the large number of archeological projects that we were collaborating on, including this example of an archeological excavation at the local Moscow High School which unearthed this paleolithic bug juice container and He-Man leg. Examples of archeological context photos where we needed to extract the object from it's background All of these projects were controlled context photographs of the object beside a ruler, possibly a color swatch, with varying backgrounds and lighting setups. The idea was to identify and extract the objects and, since different fellowship collaborators wanted these objects to be reproduced with both white and black backgrounds, we thought providing both of these options as well as a PNG file with a transparent background would work best. edge_detector Python script with explanation of import functions The tool implements a neural network called IS-Net, originally developed for a paper titled “Highly Accurate Dichotomous Image Segmentation” by Xuebin Qin and others in 2022. The model executes fine grain, binary foreground/background segmentation and it is often used for isolating retail objects for online marketplaces. The model is completely open access, requiring the user to open and close sessions within the Python script to use, but not requiring a monetary API key. U2Net vs. isnet-general-use model computer vision There are a few iterations that can be implemented under the greater rembg (remove background) library. At first, I was using the original u2net model, but found that the isnet-general-use model is more accurate and produces finer lines around the object, if a little slower to process. Beginning of setup.md guide and folder structure on the left In both tools you’ll notice I have a fairly Python for Dummies approach to structure, with original images being dropped into folder A, transparent backed PNGs into B, white backgrounds into C and black into D. After cloning this repository in GitHub, you can just follow the steps outlined in the setup.md file for either Mac or Windows users to use these scripts."
    },
    { 
        "url": "/vra_2025/content/3_ie.html",
        "title": "image_extractor tool",
        "text": "Page from the Tacoma Mountaineers 1911 scrapbook, courtesy Tacoma Northwest Room. The second tool was prompted by an archivist digitizing scrapbooks with our Zeutschel overhead scanner and wondering if we could batch extract individual photographs from the full page images. The image processing here is done using Scikit-Image, a collection of algorithms that can be implemented in different ways depending on the type of content you need to mask and extract. image_extractor Python code, including initial imports, customizable parameters and image preprocessing In this script, I am using Scikit’s io and color to preprocess images, making them easier to identify. Then this uses their morphology, closing and remove_small_objects to reduce noise, identify the correct shapes within the image to extract and then fill in the holes within the objects if anything is missing with the original visual resource data. The design of this code is a little different than the previous tool, with simple, human readable parameters you can adjust depending on the general size of the photos in their collection, the number of photos within an image and the amount of margin they would like their photos to have around the photos."
    },
    { 
        "url": "/vra_2025/content/4_findings.html",
        "title": "Findings",
        "text": "edge_detector successful applications Despite the Edge Detection tool working with much more complex materials than the Image Extraction tool, the results are much more accurate. I believe this is due to the model being trained to identify retail objects at close range, with fairly blank backgrounds on newer cameras – which just so happens to be very close to the same conditions of archeological context photos. edge_detector unsuccessful examples On the 24 objects I tested with this tool, including some tricky, nearly transparent glass items, the tool only produced three minor inaccuracies: a very slight shadow beside a glass shard, the ruler next to this pencil, possibly due to proximity and the text being lifted from a label you can view on the banner for this presentation. image_extractor successful example While the Image Extraction tool is simply identifying rectangles within an image, the action is complicated with variable margins, paper backing color, overlapping or connected images as well as patterns on the actual scrapbook paper. On a scrapbook with a plain background and fairly straight somewhat evenly spaced photos, it identified and correctly extracted 25 of the 26 photos, although there are still minor inaccuracies around the margins of the photographs – which is something you can adjust for manually in the parameters section mentioned earlier. image_extractor least successful examples On a scrapbook containing the above complications, the tool only identified and extracted 48 out of the 56 total items. That said, the tool is still certainly a work in progress and you can follow (and suggest) improvements on the GitHub public repository. About the Author Andrew Weymouth is the Digital Initiatives Librarian at University of Idaho, primarily focusing on static web design to curate the institution’s special collections and partner with faculty and graduate students on fellowship projects. He has also created digital scholarship projects for the universities of Oregon, Washington and the Tacoma Northwest Room archives, ranging from long form audio public history to architectural databases and network visualizations. He writes about labor, architecture, underrepresented communities and using digital scholarship methods to survey equity in archival collections. More Workshops from the Author"
    },
    { 
        "url": "/vra_2025/",
        "title": "Home",
        "text": "Image Extraction for Overhead Scans and Archeological Photographs slide deck Presentation for the 2025 Visual Resources Association Conference. Contents: Introduction edge_detector tool image_extractor tool Findings Content: CC BY-NC-ND 4.0 Andrew Weymouth 2025 (get source code). Theme: Variation on workshop-template-b by evanwill"
    }];
